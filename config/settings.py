"""Project configuration, paths, and processing limits."""

from __future__ import annotations

from pathlib import Path

# ============================================================================
# PROJECT PATHS
# ============================================================================

# Project root: .../offer_creation
PROJECT_ROOT = Path(__file__).resolve().parents[2]

# Input / Output directories
INPUT_ROOT = PROJECT_ROOT / "input_offers"
OUTPUT_ROOT = PROJECT_ROOT / "offer_outputs"

FOOD_INPUT_DIR = INPUT_ROOT / "food"
HPC_INPUT_DIR = INPUT_ROOT / "hpc"

# DEV: Keep input files in place (don't move to processed)
MOVE_INPUT_TO_PROCESSED = False

# ============================================================================
# FILE UPLOAD LIMITS
# ============================================================================

# Maximum file size in MB (quick filter before processing)
MAX_FILE_SIZE_MB = 50

# Excel content limits (per sheet)
MAX_SHEET_ROWS = 10_000   # 10k rows (realistic for supplier offers)
MAX_SHEET_COLS = 100      # 100 columns (more than enough)
MAX_SHEETS = 20           # Maximum number of sheets to display in UI

# Hard block extremely wide sheets (these will crash openpyxl/pandas)
EXTREME_COLS_LIMIT = 500  # Block sheets with >500 columns

# ============================================================================
# LLM PROCESSING LIMITS
# ============================================================================

# Maximum text characters before sending to LLM
# Roughly 120k chars ‚âà 30k tokens (conservative estimate)
MAX_TEXT_CHARS_BEFORE_LLM = 120_000

# JSON retry/repair attempts when LLM returns invalid JSON
JSON_RETRY_ATTEMPTS = 3

# Chunk size for large files (rows per chunk)
CHUNK_SIZE = 50

# ============================================================================
# LLM SETTINGS
# ============================================================================

DEFAULT_MODEL = "gpt-4o-mini"
DEFAULT_TEMPERATURE = 0

# ============================================================================
# USAGE NOTES
# ============================================================================

# These limits are designed to:
# 1. Prevent out-of-memory crashes on Streamlit Cloud (RAM limits)
# 2. Prevent LLM token limit errors (context window)
# 3. Provide clear user feedback before processing starts
# 4. Allow processing of most real-world supplier offers

# Typical file sizes that work:
# ‚úÖ 100-500 rows, 10-50 columns ‚Üí processes in seconds
# ‚úÖ 1,000-5,000 rows, 20-100 columns ‚Üí processes in 1-2 minutes
# ‚ö†Ô∏è 10,000+ rows ‚Üí may need chunking
# üö´ 100,000+ rows or 1,000+ columns ‚Üí too large, user must filter/split

# To adjust limits:
# - Increase MAX_SHEET_ROWS if you have more RAM (e.g., self-hosted)
# - Decrease CHUNK_SIZE if hitting token limits
# - Increase JSON_RETRY_ATTEMPTS if dealing with unreliable LLM output